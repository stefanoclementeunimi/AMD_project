{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefanoclementeunimi/AMD_project/blob/main/project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-MrJnFG43nW"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!tar xvf spark-3.4.1-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KZFmy_h13mo-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3hV4ssJ7QUO"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark\n",
        "\n",
        "import findspark\n",
        "findspark.init(\"/content/spark-3.4.1-bin-hadoop3\")\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "                    .master('local[*]') \\\n",
        "                    .appName('Basics') \\\n",
        "                    .config('spark.executor.memory', '8g') \\\n",
        "                    .config(\"spark.memory.offHeap.enabled\",\"true\")\\\n",
        "                    .config(\"spark.memory.offHeap.size\",\"16g\") \\\n",
        "                    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26vzVIJ44N0k"
      },
      "outputs": [],
      "source": [
        "# Connection & download\n",
        "os.environ['KAGGLE_USERNAME'] = \"XXX\"\n",
        "os.environ['KAGGLE_KEY'] = \"XXX\"\n",
        "!kaggle datasets download -d yelp-dataset/yelp-dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JKaGeMzrOkho"
      },
      "outputs": [],
      "source": [
        "# The academic dataset review is extracted from the zip file\n",
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile('yelp-dataset.zip', 'r') as f:\n",
        "  f.extract('yelp_academic_dataset_review.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1LImHcid4YNy"
      },
      "outputs": [],
      "source": [
        "# We import here all the pyspark/python stuff we need\n",
        "\n",
        "import pyspark\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "import string, hashlib\n",
        "from random import randint\n",
        "\n",
        "# Default values for our algorithm\n",
        "hashNum = 20        # Number of hashes in a signature\n",
        "bandW = 2           # Width of our bands, must divide hashNum\n",
        "threshold = 0.1     # Threshold for our similar pair\n",
        "limit_flag = True   # If limit_flag is set to True, we use a fraction of the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The json dataset is turned into a pyspark dataframe.\n",
        "\n",
        "sp_df = spark.read.json('yelp_academic_dataset_review.json')\n",
        "\n",
        "# We index every tweet with its row number, which we call document_id.\n",
        "# We keep both the tweets and their document id.\n",
        "\n",
        "if limit_flag:\n",
        "  sp_df = sp_df.select('text')\\\n",
        "               .withColumn('document_id', F.monotonically_increasing_id())\\\n",
        "               .limit(10000)\n",
        "else:\n",
        "  sp_df = sp_df.select('text')\\\n",
        "               .withColumn('document_id', F.monotonically_increasing_id())\n",
        "\n",
        "# We drop all the missing values from the text field\n",
        "sp_df = sp_df.dropna()"
      ],
      "metadata": {
        "id": "-D3HNj4vOxyT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7WpxNqNwyqri"
      },
      "outputs": [],
      "source": [
        "# This function removes all the punctuation symbols.\n",
        "\n",
        "def remove_punctuation(text):\n",
        "  return text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
        "\n",
        "\n",
        "# This function replaces the text field with the set of all the shingles in a document.\n",
        "# We define a shingle as any three consecutive words in a text.\n",
        "# Furthermore, our shingles will be hashed into 32-bit integers.\n",
        "\n",
        "def make_shingles(text):\n",
        "  shingles = set()\n",
        "  hashed_shingles = []\n",
        "  tweet = text.split()\n",
        "  length = len(tweet)\n",
        "  if (length >= 3):\n",
        "    for i in range(length - 2):\n",
        "      shingles.add(\" \".join([tweet[i], tweet[i+1], tweet[i+2]]))\n",
        "  else:\n",
        "    shingles.add(text)\n",
        "  hashed_shingles = [int.from_bytes(hashlib.sha256(bytes(word, encoding='UTF-8')).digest()[:4], 'little') for word in shingles]\n",
        "  return hashed_shingles\n",
        "\n",
        "# In order to use them in a Spark context, we need to convert them into vectorial functions (UDFs)\n",
        "\n",
        "make_shinglesUDF = F.udf(lambda x: make_shingles(x), ArrayType(LongType()))\n",
        "remove_punctuationUDF = F.udf(lambda x: remove_punctuation(x), StringType())\n",
        "\n",
        "# Finally, we apply the text processing functions to our dataset.\n",
        "\n",
        "df_processed = sp_df.select('document_id', remove_punctuationUDF(F.col('text')).alias('text'))\\\n",
        "                .select('document_id', make_shinglesUDF(F.col('text')).alias('text'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2zOoLj2ycKDH"
      },
      "outputs": [],
      "source": [
        "# Now we want to apply the MinHash algorithm to our dataframe.\n",
        "# As a first step, we need to produce a family of n hash functions.\n",
        "# A common strategy is to use functions of the form h(x) = (a*x + b) % c\n",
        "# a and b will be chosen at random according to our needs\n",
        "# c is a prime slighty bigger than the maximum value of the shingles - which is 2^32-1.\n",
        "\n",
        "\n",
        "maxShingleValue = 2**32-1\n",
        "prime = 4294967311 # prime is the smallest prime bigger than 2^32-1\n",
        "\n",
        "def generate_random_values(n, maxValue):\n",
        "  coeff = []\n",
        "  while len(coeff) < n:\n",
        "    r = randint(0, maxValue)\n",
        "    if r not in coeff:\n",
        "      coeff.append(r)\n",
        "  return coeff\n",
        "\n",
        "def generate_hash_function(n=hashNum, maxValue=maxShingleValue, c=prime):\n",
        "  hashFunctions = []\n",
        "  a_coeff = generate_random_values(n, maxValue)\n",
        "  b_coeff = generate_random_values(n, maxValue)\n",
        "  for i in range(n):\n",
        "    hashFunctions.append(lambda x, i=i: (a_coeff[i]*x + b_coeff[i]) % c)\n",
        "  return hashFunctions\n",
        "\n",
        "# This function calculates the signature for every document\n",
        "\n",
        "def min_hash(shingles, hash_list):\n",
        "  hashed_shingles = []\n",
        "  for hash in hash_list:\n",
        "    values = list(map(hash, shingles))\n",
        "    min_h = min(values, default=4294967312)\n",
        "    hashed_shingles.append(min_h)\n",
        "  return hashed_shingles\n",
        "\n",
        "# We generate the list of hashes functions.\n",
        "hashes = generate_hash_function()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BAzWAPjZkFLq"
      },
      "outputs": [],
      "source": [
        "# Finally, MinHash is applied. We obtain a dataframe where each document has its own signature\n",
        "\n",
        "min_hashUDF = F.udf(lambda x: min_hash(x, hashes), ArrayType(LongType()))\n",
        "\n",
        "df_minhashed = df_processed.select('document_id', min_hashUDF(F.col('text')).alias('text'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Having calculated the minHash signature, now we expand our dataset.\n",
        "# We aim to produce one row for each element of a signature.\n",
        "# We keep track of the signature indices in order\n",
        "# to match them and apply our banding technique.\n",
        "\n",
        "df_expanded = df_minhashed.select('document_id', F.posexplode('text').alias('signatureIndex', 'minHash'))\n",
        "\n"
      ],
      "metadata": {
        "id": "XoAlYfleT1hR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we're using LSH: first we split our dataset into bands.\n",
        "# A band is a subset of indices, reasonably dimensioned.\n",
        "# Then, within that band, we collect a slice of the signature for each document\n",
        "# and hash it.\n",
        "\n",
        "def verify_pairs(nodeList):\n",
        "  return len(nodeList) > 1\n",
        "\n",
        "verify_pairsUDF = F.udf(lambda x: verify_pairs(x), BooleanType())\n",
        "\n",
        "def lsh_banding(df, indices):\n",
        "  df = df.filter(F.col('signatureIndex').isin(indices))\n",
        "  df = df.groupby('document_id').agg(F.collect_list(F.col('minHash')).alias('reducedSignature'))\\\n",
        "         .withColumn('hashedSignature', F.hash(F.col('reducedSignature')))\\\n",
        "         .groupBy('hashedSignature').agg(F.collect_list(F.col('document_id')).alias('similar_documents'))\n",
        "  df = df.withColumn('candidates', verify_pairsUDF(F.col('similar_documents')))\\\n",
        "         .filter(F.col('candidates') == True)\n",
        "  df = df.withColumnRenamed('similar_documents', 'first_list')\n",
        "  df_j = df.withColumnRenamed('first_list', 'second_list')\n",
        "  df = df.join(df_j, ['hashedSignature'], 'inner')\\\n",
        "         .select(F.explode(F.col('first_list')).alias('first_document'), F.col('second_list'))\\\n",
        "         .select(F.col('first_document'), F.explode(F.col('second_list')).alias('second_document'))\\\n",
        "         .filter(F.when(F.col('first_document') == F.col('second_document'), False).otherwise(True))\n",
        "  return df\n",
        "\n",
        "# This function groups together all the candidates\n",
        "# that won the LSH round.\n",
        "# Later, we will compute explicitly their Jaccard Similarity.\n",
        "\n",
        "def build_candidate_list_lsh(df, hashNum, bandW):\n",
        "  indices = [list(range(hashNum))[i:i + bandW] for i in range(0, len(list(range(hashNum))), bandW)]\n",
        "  # Caching would be appropriate here\n",
        "  df_un = lsh_banding(df, indices[0])\n",
        "  for index_el in indices[1:]:\n",
        "    df_un = df_un.union(lsh_banding(df, index_el))\n",
        "  return df_un.dropDuplicates()\n",
        "\n",
        "lsh_candidates = build_candidate_list_lsh(df_expanded, hashNum, bandW)"
      ],
      "metadata": {
        "id": "hy4aEyJMXvdQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Having at our disposal all the candidates, now we are able to restrict our research\n",
        "# to the promising documents.\n",
        "\n",
        "\n",
        "# First, we group by the ordered pair (index, hash), collecting\n",
        "# sets of elements that share a particular signature.\n",
        "\n",
        "df_coupled = df_expanded.groupby('signatureIndex', 'minHash')\\\n",
        "                        .agg(F.collect_set(F.col('document_id')).alias('firstList'),\n",
        "                             F.collect_set(F.col('document_id')).alias('secondList'))\n",
        "\n",
        "# This final query is a standard way to compute the Jaccard Similarity\n",
        "\n",
        "df_coupled = df_coupled.withColumnRenamed('firstList', \"first_document\")\n",
        "df_coupled_j = df_coupled.withColumnRenamed('first_document', 'second_document')\n",
        "\n",
        "final_df= df_coupled.join(df_coupled_j, ['signatureIndex', 'minHash'], 'inner')\\\n",
        "                     .select(F.col('minhash'),\n",
        "                             F.explode(F.col('first_document')).alias('first_document'),\n",
        "                             F.col('second_document'))\n",
        "final_df = final_df.select(F.col('minHash'),\n",
        "                           F.col('first_document'),\n",
        "                           F.explode(F.col('second_document')).alias('second_document'))\n",
        "\n",
        "# Here we restrict our search (before the aggregation) to our candidate pairs\n",
        "\n",
        "final_df = final_df.join(lsh_candidates, ['first_document', 'second_document'], 'inner')\\\n",
        "                   .groupby('first_document', 'second_document')\\\n",
        "                   .agg((F.count('*') / hashNum).alias('jaccard_similarity'))\\\n",
        "                   .filter(F.when(F.col('first_document') == F.col('second_document'), False).otherwise(True))\n",
        "final_df = final_df.filter(F.col('jaccard_similarity') > F.lit(threshold))\n",
        "\n",
        "\n",
        "final_df = final_df.withColumn(\"Pair\", F.array(F.col(\"first_document\"), F.col(\"second_document\")))\\\n",
        "                   .withColumn(\"Pair\", F.array_sort(F.col(\"Pair\")))\\\n",
        "                   .dropDuplicates(['Pair'])\\\n",
        "                   .select(\"Pair\", \"jaccard_similarity\")\n"
      ],
      "metadata": {
        "id": "N7h8Bz5mOtB_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.show()"
      ],
      "metadata": {
        "id": "nqPJwVbiQn2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccade489-003e-4249-df80-364e66b6b057"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------------+\n",
            "|        Pair|jaccard_similarity|\n",
            "+------------+------------------+\n",
            "|[1311, 8877]|               0.2|\n",
            "|[1982, 4971]|              0.15|\n",
            "|[1026, 6231]|              0.25|\n",
            "|[3131, 8113]|              0.15|\n",
            "| [308, 5108]|              0.15|\n",
            "| [339, 7197]|               0.2|\n",
            "|[6373, 8860]|               0.2|\n",
            "|[6202, 9198]|              0.15|\n",
            "|[4054, 6231]|              0.15|\n",
            "|[1609, 8548]|              0.15|\n",
            "| [396, 1602]|               0.2|\n",
            "|[1026, 2649]|              0.25|\n",
            "|[6197, 9198]|              0.15|\n",
            "|[1595, 2961]|              0.15|\n",
            "|[4584, 7273]|              0.15|\n",
            "|[6231, 7885]|              0.15|\n",
            "| [553, 8163]|              0.15|\n",
            "|[2085, 4881]|               0.2|\n",
            "|[5268, 7859]|              0.15|\n",
            "| [553, 6373]|              0.15|\n",
            "+------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A csv document containing all the found pairs is stored in the content directory\n",
        "final_df.withColumn(\"Pair\", F.col(\"Pair\").cast(\"string\")).write.csv(\"/content/pairs.csv\")\n"
      ],
      "metadata": {
        "id": "HzAU7Pbrbb6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TGhX4ruMCz7M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M2Wbcy1NdFX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsXAlR9pcnxDvSFHxWvvEU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}